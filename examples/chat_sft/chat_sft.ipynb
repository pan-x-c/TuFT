{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364b8882",
   "metadata": {},
   "source": [
    "# TuFT-Chat Supervised Fine-Tuning\n",
    "\n",
    "This notebook:\n",
    "- Connects to a running TuFT server\n",
    "- Loads a chat dataset (no_robots)\n",
    "- Tokenizes conversations with loss masking\n",
    "- Trains a LoRA\n",
    "- Evaluates and plots loss\n",
    "- Saves weights/checkpoints and runs a sample generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d013b",
   "metadata": {},
   "source": [
    "## 0) Prerequisites\n",
    "the experiments below were conducted on a local 2Ã— NVIDIA A100-SXM4-80GB setup (Driver 550.54.15, CUDA 12.9).\n",
    "- TuFT server running (e.g. `http://localhost:10610`)\n",
    "- API key available\n",
    "- Dependencies installed: `tinker datasets transformers matplotlib numpy`\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "43b8c68f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-26T09:41:43.342413Z",
     "iopub.status.busy": "2026-01-26T09:41:43.342287Z",
     "iopub.status.idle": "2026-01-26T09:41:47.972003Z",
     "shell.execute_reply": "2026-01-26T09:41:47.971584Z",
     "shell.execute_reply.started": "2026-01-26T09:41:43.342398Z"
    },
    "tags": []
   },
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tinker\n",
    "from datasets import load_dataset\n",
    "from tinker import types\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e19c02ec",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "1ab2aaeb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-26T09:41:47.973177Z",
     "iopub.status.busy": "2026-01-26T09:41:47.972835Z",
     "iopub.status.idle": "2026-01-26T09:41:47.977071Z",
     "shell.execute_reply": "2026-01-26T09:41:47.976626Z",
     "shell.execute_reply.started": "2026-01-26T09:41:47.973162Z"
    },
    "tags": []
   },
   "source": [
    "# TuFT server\n",
    "TINKER_BASE_URL = \"http://localhost:10610\"\n",
    "TINKER_API_KEY = os.getenv(\"TINKER_API_KEY\")\n",
    "\n",
    "# Dataset\n",
    "DATASET = \"no_robots\"\n",
    "\n",
    "# Model\n",
    "BASE_MODEL = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "LORA_RANK = 16\n",
    "\n",
    "# Training\n",
    "NUM_STEPS = 50\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "print(\"=== CONFIG ===\")\n",
    "print(\"Server:\", TINKER_BASE_URL)\n",
    "print(\"Dataset:\", DATASET)\n",
    "print(\"Base model:\", BASE_MODEL)\n",
    "print(\"LoRA rank:\", LORA_RANK)\n",
    "print(\"Steps:\", NUM_STEPS, \"Batch:\", BATCH_SIZE, \"LR:\", LEARNING_RATE, \"MaxLen:\", MAX_LENGTH)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "68427f6a",
   "metadata": {},
   "source": "## 2) Connect to TuFT server"
  },
  {
   "cell_type": "code",
   "id": "ca95b205",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:41:47.977615Z",
     "iopub.status.busy": "2026-01-26T09:41:47.977481Z",
     "iopub.status.idle": "2026-01-26T09:41:48.038754Z",
     "shell.execute_reply": "2026-01-26T09:41:48.038307Z",
     "shell.execute_reply.started": "2026-01-26T09:41:47.977603Z"
    },
    "tags": []
   },
   "source": [
    "service_client = tinker.ServiceClient(base_url=TINKER_BASE_URL, api_key=TINKER_API_KEY)\n",
    "print(\"ServiceClient created:\", service_client)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dedf664d",
   "metadata": {},
   "source": [
    "## 3) Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "id": "5a67f323",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-26T09:41:48.039589Z",
     "iopub.status.busy": "2026-01-26T09:41:48.039357Z",
     "iopub.status.idle": "2026-01-26T09:41:49.561223Z",
     "shell.execute_reply": "2026-01-26T09:41:49.560600Z",
     "shell.execute_reply.started": "2026-01-26T09:41:48.039574Z"
    },
    "tags": []
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "print(\"Tokenizer loaded:\", type(tokenizer).__name__)\n",
    "print(\"eos_token_id:\", tokenizer.eos_token_id)\n",
    "\n",
    "_ids = tokenizer.encode(\"hello\", add_special_tokens=False)\n",
    "print(\"Sanity encode('hello') ids[:5] =\", _ids[:5], \"len=\", len(_ids))\n",
    "print(\"Sanity decode =\", tokenizer.decode(_ids))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d7ed3fc",
   "metadata": {},
   "source": [
    "## 4) Tokenization utilities"
   ]
  },
  {
   "cell_type": "code",
   "id": "a752fd41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:41:49.561950Z",
     "iopub.status.busy": "2026-01-26T09:41:49.561795Z",
     "iopub.status.idle": "2026-01-26T09:41:49.579356Z",
     "shell.execute_reply": "2026-01-26T09:41:49.578996Z",
     "shell.execute_reply.started": "2026-01-26T09:41:49.561937Z"
    },
    "tags": []
   },
   "source": [
    "def tokenize_conversation(messages, tokenizer, max_length: int):\n",
    "    \"\"\"Tokenize a conversation and compute per-token loss weights.\"\"\"\n",
    "    all_tokens = []\n",
    "    all_weights = []\n",
    "\n",
    "    for i, msg in enumerate(messages):\n",
    "        partial = messages[: i + 1]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            partial,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "        prev_len = len(all_tokens)\n",
    "        new_tokens = tokens[prev_len:]\n",
    "\n",
    "        is_assistant = msg.get(\"role\") == \"assistant\"\n",
    "        weight = 1.0 if is_assistant else 0.0\n",
    "\n",
    "        all_tokens.extend(new_tokens)\n",
    "        all_weights.extend([weight] * len(new_tokens))\n",
    "\n",
    "    if len(all_tokens) > max_length:\n",
    "        all_tokens = all_tokens[:max_length]\n",
    "        all_weights = all_weights[:max_length]\n",
    "\n",
    "    return all_tokens, np.array(all_weights, dtype=np.float32)\n",
    "\n",
    "\n",
    "def conversation_to_datum(messages, tokenizer, max_length: int) -> types.Datum:\n",
    "    \"\"\"Convert a conversation into next-token-prediction Datum with shifted weights.\"\"\"\n",
    "    tokens, weights = tokenize_conversation(messages, tokenizer, max_length)\n",
    "    if len(tokens) < 2:\n",
    "        raise ValueError(\"Conversation too short\")\n",
    "\n",
    "    input_tokens = tokens[:-1]\n",
    "    target_tokens = tokens[1:]\n",
    "    target_weights = weights[1:]\n",
    "\n",
    "    return types.Datum(\n",
    "        model_input=types.ModelInput.from_ints(input_tokens),\n",
    "        loss_fn_inputs={\n",
    "            \"target_tokens\": list(target_tokens),\n",
    "            \"weights\": target_weights.tolist(),\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "# Verification\n",
    "_debug_msgs = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi, how can I help you?\"},\n",
    "]\n",
    "_debug_tokens, _debug_w = tokenize_conversation(_debug_msgs, tokenizer, MAX_LENGTH)\n",
    "print(\"Tokenization sanity:\")\n",
    "print(\"  tokens_len =\", len(_debug_tokens))\n",
    "print(\"  weights_len =\", len(_debug_w))\n",
    "print(\"  weight_sum =\", float(_debug_w.sum()), \"nonzero =\", int((_debug_w > 0).sum()))\n",
    "print(\"  weight_unique =\", sorted(set(_debug_w.tolist())))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e598a16",
   "metadata": {},
   "source": [
    "## 5) Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "id": "2396d5ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:41:49.579908Z",
     "iopub.status.busy": "2026-01-26T09:41:49.579771Z",
     "iopub.status.idle": "2026-01-26T09:41:53.631967Z",
     "shell.execute_reply": "2026-01-26T09:41:53.631558Z",
     "shell.execute_reply.started": "2026-01-26T09:41:49.579896Z"
    },
    "tags": []
   },
   "source": [
    "@dataclass\n",
    "class ChatDataset:\n",
    "    \"\"\"Simple chat dataset with batching.\"\"\"\n",
    "\n",
    "    data: list[list[dict]]\n",
    "    index: int = 0\n",
    "\n",
    "    def get_batch(self, batch_size: int) -> list[list[dict]]:\n",
    "        batch = []\n",
    "        for _ in range(batch_size):\n",
    "            if self.index >= len(self.data):\n",
    "                self.index = 0\n",
    "                random.shuffle(self.data)\n",
    "            batch.append(self.data[self.index])\n",
    "            self.index += 1\n",
    "        return batch\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def load_chat_dataset(dataset_name: str, seed: int = 42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    if dataset_name == \"no_robots\":\n",
    "        ds = load_dataset(\"HuggingFaceH4/no_robots\")\n",
    "        train_data = [row[\"messages\"] for row in ds[\"train\"]]\n",
    "        test_data = [row[\"messages\"] for row in ds[\"test\"]]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "    random.shuffle(train_data)\n",
    "    return ChatDataset(train_data), ChatDataset(test_data)\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = load_chat_dataset(DATASET)\n",
    "print(f\"Dataset loaded: {DATASET}\")\n",
    "print(f\"Train: {len(train_dataset)} conversations\")\n",
    "print(f\"Test:  {len(test_dataset)} conversations\")\n",
    "\n",
    "sample = train_dataset.get_batch(1)[0]\n",
    "print(f\"\\nSample conversation ({len(sample)} messages), showing first 3:\")\n",
    "for msg in sample[:3]:\n",
    "    content = msg[\"content\"][:100] + (\"...\" if len(msg[\"content\"]) > 100 else \"\")\n",
    "    print(f\"  [{msg['role']}]: {content}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7424543f",
   "metadata": {},
   "source": [
    "## 6) Create LoRA training client"
   ]
  },
  {
   "cell_type": "code",
   "id": "764cfa82",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-26T09:41:53.633118Z",
     "iopub.status.busy": "2026-01-26T09:41:53.632954Z",
     "iopub.status.idle": "2026-01-26T09:41:54.064950Z",
     "shell.execute_reply": "2026-01-26T09:41:54.064386Z",
     "shell.execute_reply.started": "2026-01-26T09:41:53.633104Z"
    },
    "tags": []
   },
   "source": [
    "training_client = service_client.create_lora_training_client(\n",
    "    base_model=BASE_MODEL,\n",
    "    rank=LORA_RANK,\n",
    "    train_mlp=True,\n",
    "    train_attn=True,\n",
    "    train_unembed=True,\n",
    ")\n",
    "\n",
    "print(\"Training client created:\", training_client)\n",
    "print(\"LoRA config:\", {\"base_model\": BASE_MODEL, \"rank\": LORA_RANK})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cdb7d804",
   "metadata": {},
   "source": [
    "## 7) Train loop (SFT)"
   ]
  },
  {
   "cell_type": "code",
   "id": "69a1feb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:41:54.065735Z",
     "iopub.status.busy": "2026-01-26T09:41:54.065565Z",
     "iopub.status.idle": "2026-01-26T09:43:01.093741Z",
     "shell.execute_reply": "2026-01-26T09:43:01.093299Z",
     "shell.execute_reply.started": "2026-01-26T09:41:54.065721Z"
    },
    "scrolled": true,
    "tags": []
   },
   "source": [
    "def compute_weighted_nll_from_outputs(loss_fn_outputs, datums):\n",
    "    total_loss = 0.0\n",
    "    total_weight = 0.0\n",
    "    for i, out in enumerate(loss_fn_outputs):\n",
    "        logprobs = out[\"logprobs\"]\n",
    "        if hasattr(logprobs, \"tolist\"):\n",
    "            logprobs = logprobs.tolist()\n",
    "\n",
    "        w = datums[i].loss_fn_inputs[\"weights\"]\n",
    "        if hasattr(w, \"tolist\"):\n",
    "            w = w.tolist()\n",
    "\n",
    "        for lp, wt in zip(logprobs, w, strict=True):\n",
    "            total_loss += -lp * wt\n",
    "            total_weight += wt\n",
    "    return total_loss / max(total_weight, 1.0)\n",
    "\n",
    "\n",
    "metrics_history = []\n",
    "\n",
    "print(f\"Starting SFT training: {NUM_STEPS} steps\")\n",
    "print(f\"Batch: {BATCH_SIZE}, LR: {LEARNING_RATE}\\n\")\n",
    "\n",
    "for step in range(NUM_STEPS):\n",
    "    batch = train_dataset.get_batch(BATCH_SIZE)\n",
    "\n",
    "    datums = []\n",
    "    for messages in batch:\n",
    "        try:\n",
    "            datums.append(conversation_to_datum(messages, tokenizer, MAX_LENGTH))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    print(f\"[debug] step={step} valid_datums={len(datums)}/{BATCH_SIZE}\")\n",
    "    if not datums:\n",
    "        continue\n",
    "\n",
    "    fwdbwd_result = training_client.forward_backward(\n",
    "        datums,\n",
    "        loss_fn=\"cross_entropy\",\n",
    "    ).result()\n",
    "\n",
    "    loss = compute_weighted_nll_from_outputs(fwdbwd_result.loss_fn_outputs, datums)\n",
    "\n",
    "    training_client.optim_step(types.AdamParams(learning_rate=LEARNING_RATE)).result()\n",
    "\n",
    "    metrics_history.append({\"step\": step, \"loss\": loss})\n",
    "\n",
    "    if step % 10 == 0 or step == NUM_STEPS - 1:\n",
    "        print(f\"Step {step:3d}: loss={loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "if metrics_history:\n",
    "    print(f\"Initial loss: {metrics_history[0]['loss']:.4f}\")\n",
    "    print(f\"Final loss:   {metrics_history[-1]['loss']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3efa02e",
   "metadata": {},
   "source": [
    "## 8) Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "id": "aba55973",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:43:01.094378Z",
     "iopub.status.busy": "2026-01-26T09:43:01.094212Z",
     "iopub.status.idle": "2026-01-26T09:43:01.687695Z",
     "shell.execute_reply": "2026-01-26T09:43:01.687258Z",
     "shell.execute_reply.started": "2026-01-26T09:43:01.094365Z"
    },
    "tags": []
   },
   "source": [
    "test_batch = test_dataset.get_batch(min(16, len(test_dataset)))\n",
    "\n",
    "test_datums = []\n",
    "for messages in test_batch:\n",
    "    try:\n",
    "        test_datums.append(conversation_to_datum(messages, tokenizer, MAX_LENGTH))\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "print(\"Test valid_datums:\", len(test_datums), \"/\", len(test_batch))\n",
    "\n",
    "if test_datums:\n",
    "    forward_result = training_client.forward(\n",
    "        test_datums,\n",
    "        loss_fn=\"cross_entropy\",\n",
    "    ).result()\n",
    "\n",
    "    test_loss = compute_weighted_nll_from_outputs(forward_result.loss_fn_outputs, test_datums)\n",
    "    print(f\"Test NLL: {test_loss:.4f}\")\n",
    "else:\n",
    "    print(\"No valid test samples\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f7d2c5c7",
   "metadata": {},
   "source": [
    "## 9) Plot training loss"
   ]
  },
  {
   "cell_type": "code",
   "id": "eef11ba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T09:43:01.688360Z",
     "iopub.status.busy": "2026-01-26T09:43:01.688212Z",
     "iopub.status.idle": "2026-01-26T09:43:01.926815Z",
     "shell.execute_reply": "2026-01-26T09:43:01.926304Z",
     "shell.execute_reply.started": "2026-01-26T09:43:01.688348Z"
    },
    "tags": []
   },
   "source": [
    "if metrics_history:\n",
    "    steps = [m[\"step\"] for m in metrics_history]\n",
    "    losses = [m[\"loss\"] for m in metrics_history]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(steps, losses, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss (NLL)\")\n",
    "    plt.title(f\"{DATASET.upper()} SFT Training\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_path = f\"{DATASET}_training.png\"\n",
    "    plt.savefig(fig_path, dpi=150)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b7f24f5",
   "metadata": {},
   "source": [
    "## 10) Save weights + sample generation + save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8133b85",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-26T09:43:01.927511Z",
     "iopub.status.busy": "2026-01-26T09:43:01.927346Z",
     "iopub.status.idle": "2026-01-26T09:43:04.046789Z",
     "shell.execute_reply": "2026-01-26T09:43:04.046351Z",
     "shell.execute_reply.started": "2026-01-26T09:43:01.927498Z"
    },
    "tags": []
   },
   "source": [
    "sampling_path = training_client.save_weights_for_sampler(name=f\"{DATASET}-sft-demo\").result().path\n",
    "\n",
    "# print(\"Sampling checkpoint path:\", sampling_path)\n",
    "\n",
    "sampling_client = service_client.create_sampling_client(model_path=sampling_path)\n",
    "print(\"Sampling client created:\", sampling_client)\n",
    "\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"Write a haiku about programming.\"}]\n",
    "\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "prompt_tokens = tokenizer.encode(prompt_text, add_special_tokens=False)\n",
    "print(\"Prompt tokens len:\", len(prompt_tokens))\n",
    "\n",
    "sample_result = sampling_client.sample(\n",
    "    prompt=types.ModelInput.from_ints(prompt_tokens),\n",
    "    num_samples=1,\n",
    "    sampling_params=types.SamplingParams(\n",
    "        max_tokens=128,\n",
    "        temperature=0.7,\n",
    "        stop_token_ids=[tokenizer.eos_token_id],\n",
    "    ),\n",
    ")\n",
    "sample_result = sample_result.result() if hasattr(sample_result, \"result\") else sample_result\n",
    "\n",
    "response = tokenizer.decode(sample_result.sequences[0].tokens)\n",
    "\n",
    "print(\"=== Generation ===\")\n",
    "print(\"User: Write a haiku about programming.\")\n",
    "print(\"Assistant:\", response[:500])\n",
    "print(\"Response chars:\", len(response))\n",
    "\n",
    "checkpoint = training_client.save_state(name=f\"{DATASET}-sft-final\").result()\n",
    "# print(\"Checkpoint saved:\", checkpoint.path)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
