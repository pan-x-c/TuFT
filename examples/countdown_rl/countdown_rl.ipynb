{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2a1f3c",
   "metadata": {},
   "source": [
    "# TuFT-Countdown RL\n",
    "\n",
    "This notebook demonstrates a minimal RL fine-tuning workflow on the **Countdown** dataset using **TuFT**:\n",
    "\n",
    "- Load + preprocess the Countdown dataset\n",
    "- Define a reward function (format check + validity + numeric correctness)\n",
    "- Run a minimal **GRPO-like** RL loop (sampling + importance-sampling loss)\n",
    "- Periodically evaluate and optionally plot metrics + save a final checkpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5c5b1",
   "metadata": {},
   "source": [
    "## 0) Prerequisites\n",
    "the experiments below were conducted on a local 2× NVIDIA A100-SXM4-80GB setup (Driver 550.54.15, CUDA 12.9).\n",
    "\n",
    "- TuFT server running (e.g. `http://localhost:10610`)\n",
    "- API key available\n",
    "- Dependencies installed: `tinker datasets torch matplotlib`\n",
    "\n",
    "Notes:\n",
    "- If you use a mirrored Hugging Face endpoint, set `HF_ENDPOINT` before loading datasets/models.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8a8f3d2",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T11:21:36.801954Z",
     "iopub.status.busy": "2026-01-27T11:21:36.801819Z",
     "iopub.status.idle": "2026-01-27T11:21:39.252774Z",
     "shell.execute_reply": "2026-01-27T11:21:39.252358Z",
     "shell.execute_reply.started": "2026-01-27T11:21:36.801939Z"
    },
    "tags": []
   },
   "source": [
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import tinker\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from tinker import types\n",
    "from tinker.types.tensor_data import TensorData\n",
    "\n",
    "\n",
    "print(\"Imports OK: tinker =\", getattr(tinker, \"__version__\", \"unknown\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f9d2f0ef",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "dcfbf7d0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T11:21:39.253853Z",
     "iopub.status.busy": "2026-01-27T11:21:39.253552Z",
     "iopub.status.idle": "2026-01-27T11:21:39.258435Z",
     "shell.execute_reply": "2026-01-27T11:21:39.258084Z",
     "shell.execute_reply.started": "2026-01-27T11:21:39.253838Z"
    },
    "tags": []
   },
   "source": [
    "# TuFT server\n",
    "TINKER_BASE_URL = \"http://localhost:10610\"\n",
    "TINKER_API_KEY = os.getenv(\"TINKER_API_KEY\")\n",
    "\n",
    "# Dataset (Countdown)\n",
    "DATASET_NAME = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
    "\n",
    "# Model\n",
    "BASE_MODEL = \"Qwen/Qwen3-0.6B\"\n",
    "LORA_RANK = 8\n",
    "\n",
    "# Training\n",
    "NUM_STEPS = 1000\n",
    "BATCH_SIZE = 4\n",
    "GROUP_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_TOKENS = 128\n",
    "\n",
    "# Train sampling\n",
    "TEMPERATURE = 0.9\n",
    "\n",
    "# Dataset split\n",
    "TEST_SIZE = 512\n",
    "SEED = 0\n",
    "\n",
    "# Reward shaping\n",
    "FORMAT_SCORE = 0.1\n",
    "USE_CONTINUOUS_SHAPING = True\n",
    "\n",
    "# Evaluation\n",
    "EVAL_EVERY = 30\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_GROUP_SIZE = 1\n",
    "EVAL_TEMPERATURE = 0.1\n",
    "REWARD_EMA_ALPHA = 0.1\n",
    "\n",
    "\n",
    "print(\"=== CONFIG ===\")\n",
    "print(\"Server:\", TINKER_BASE_URL)\n",
    "print(\"Dataset:\", DATASET_NAME)\n",
    "print(\"Base model:\", BASE_MODEL)\n",
    "print(\"LoRA rank:\", LORA_RANK)\n",
    "print(\n",
    "    \"Steps:\",\n",
    "    NUM_STEPS,\n",
    "    \"Batch:\",\n",
    "    BATCH_SIZE,\n",
    "    \"Group:\",\n",
    "    GROUP_SIZE,\n",
    "    \"LR:\",\n",
    "    LEARNING_RATE,\n",
    "    \"MaxTokens:\",\n",
    "    MAX_TOKENS,\n",
    ")\n",
    "print(\"Train temp:\", TEMPERATURE, \"| Eval temp:\", EVAL_TEMPERATURE)\n",
    "print(\"Continuous shaping:\", USE_CONTINUOUS_SHAPING)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4d7f4e2a",
   "metadata": {},
   "source": [
    "## 2) Dataset loading + preprocessing\n",
    "\n",
    "We create prompt-style questions.\n",
    "\n",
    "Split policy (deterministic):\n",
    "- Test = first `TEST_SIZE` rows\n",
    "- Train = remaining rows (shuffled)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1678c3a2",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T11:21:39.259048Z",
     "iopub.status.busy": "2026-01-27T11:21:39.258871Z",
     "iopub.status.idle": "2026-01-27T11:21:43.906803Z",
     "shell.execute_reply": "2026-01-27T11:21:43.906405Z",
     "shell.execute_reply.started": "2026-01-27T11:21:39.259022Z"
    },
    "tags": []
   },
   "source": [
    "def load_countdown_splits(\n",
    "    dataset_name: str,\n",
    "    split: str,\n",
    "    test_size: int,\n",
    "    seed: int,\n",
    ") -> tuple[Dataset, Dataset]:\n",
    "    \"\"\"Load Countdown dataset and build prompt-style question strings.\"\"\"\n",
    "\n",
    "    ds = datasets.load_dataset(dataset_name, split=split)\n",
    "    if len(ds) <= test_size:\n",
    "        raise ValueError(f\"Dataset too small: len={len(ds)} <= test_size={test_size}\")\n",
    "\n",
    "    test_ds = ds.select(range(test_size))\n",
    "    train_ds = ds.select(range(test_size, len(ds)))\n",
    "\n",
    "    def preprocess_fn(example, _idx):\n",
    "        target = int(example[\"target\"])\n",
    "        nums = list(example[\"nums\"])\n",
    "        nums_str = \", \".join(map(str, nums))\n",
    "\n",
    "        question = (\n",
    "            f\"Using the numbers {nums_str}, reach the target number {target}. \"\n",
    "            f\"You may use +, -, *, / and parentheses, and each number can only be used once. \"\n",
    "            f\"Put ONLY the final expression inside <answer>...</answer>. \"\n",
    "            f\"Example: <answer>(1+2)/3</answer>.\"\n",
    "        )\n",
    "\n",
    "        return {\"question\": question, \"target\": target, \"nums\": nums}\n",
    "\n",
    "    train_ds = train_ds.map(preprocess_fn, with_indices=True).shuffle(seed=seed)\n",
    "    test_ds = test_ds.map(preprocess_fn, with_indices=True)\n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "train_ds, test_ds = load_countdown_splits(DATASET_NAME, \"train\", TEST_SIZE, SEED)\n",
    "print(\"Dataset loaded:\", DATASET_NAME)\n",
    "print(\"Train size:\", len(train_ds))\n",
    "print(\"Test size: \", len(test_ds))\n",
    "\n",
    "print(\"\\nSample keys:\", list(train_ds[0].keys()))\n",
    "print(\"Sample question snippet:\", train_ds[0][\"question\"][:120], \"...\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2c8a5fb",
   "metadata": {},
   "source": [
    "## 3) Reward utilities\n",
    "\n",
    "Reward pipeline:\n",
    "1. Extract `<answer>...</answer>`\n",
    "2. Validate numbers are used exactly once\n",
    "3. Safely `eval()` arithmetic expressions\n",
    "4. Return a shaped reward\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "38c7a1bd",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T11:21:43.907515Z",
     "iopub.status.busy": "2026-01-27T11:21:43.907277Z",
     "iopub.status.idle": "2026-01-27T11:21:43.914704Z",
     "shell.execute_reply": "2026-01-27T11:21:43.914228Z",
     "shell.execute_reply.started": "2026-01-27T11:21:43.907501Z"
    },
    "tags": []
   },
   "source": [
    "_ANSWER_RE = re.compile(r\"<answer>(.*?)</answer>\", flags=re.DOTALL)\n",
    "_ALLOWED_EVAL_RE = re.compile(r\"^[\\d+\\-*/().\\s]+$\")\n",
    "\n",
    "\n",
    "def extract_solution(text: str) -> str | None:\n",
    "    \"\"\"Extract the last <answer>...</answer> content from a model response.\"\"\"\n",
    "\n",
    "    if \"Assistant:\" in text:\n",
    "        text = text.split(\"Assistant:\", 1)[1]\n",
    "    elif \"<|im_start|>assistant\" in text:\n",
    "        text = text.split(\"<|im_start|>assistant\", 1)[1]\n",
    "\n",
    "    matches = list(_ANSWER_RE.finditer(text))\n",
    "    if not matches:\n",
    "        return None\n",
    "    return matches[-1].group(1).strip()\n",
    "\n",
    "\n",
    "def validate_equation(equation_str: str, available_numbers: list[int]) -> bool:\n",
    "    \"\"\"Check if equation uses exactly the provided numbers (multiset match).\"\"\"\n",
    "\n",
    "    try:\n",
    "        numbers_in_eq = [int(n) for n in re.findall(r\"\\d+\", equation_str)]\n",
    "        return sorted(numbers_in_eq) == sorted(available_numbers)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def evaluate_equation(equation_str: str) -> float | None:\n",
    "    \"\"\"Safely evaluate arithmetic expression if it matches a restricted character set.\"\"\"\n",
    "\n",
    "    try:\n",
    "        if not _ALLOWED_EVAL_RE.match(equation_str):\n",
    "            return None\n",
    "        return eval(equation_str, {\"__builtins__\": None}, {})\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def compute_reward(\n",
    "    response_text: str,\n",
    "    target: int,\n",
    "    nums: list[int],\n",
    "    format_score: float,\n",
    "    use_continuous_shaping: bool,\n",
    ") -> float:\n",
    "    \"\"\"Compute reward for a Countdown response.\n",
    "\n",
    "    - 0.0 if no <answer>\n",
    "    - format_score if invalid numbers or invalid eval\n",
    "    - 1.0 if exact\n",
    "    - otherwise optionally use continuous shaping\n",
    "    \"\"\"\n",
    "\n",
    "    equation = extract_solution(response_text)\n",
    "    if equation is None:\n",
    "        return 0.0\n",
    "\n",
    "    if not validate_equation(equation, nums):\n",
    "        return float(format_score)\n",
    "\n",
    "    result = evaluate_equation(equation)\n",
    "    if result is None:\n",
    "        return float(format_score)\n",
    "\n",
    "    err = abs(result - target)\n",
    "    if err < 1e-5:\n",
    "        return 1.0\n",
    "\n",
    "    if not use_continuous_shaping:\n",
    "        return float(format_score)\n",
    "\n",
    "    shaped = format_score + (1.0 - format_score) * (1.0 / (1.0 + err))\n",
    "    return float(shaped)\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "_txt = \"A: <answer>(2*3)+7</answer>\"\n",
    "_eq = extract_solution(_txt)\n",
    "print(\"Reward sanity:\")\n",
    "print(\"  extracted =\", _eq)\n",
    "print(\"  valid_nums =\", validate_equation(_eq, [2, 3, 7]))\n",
    "print(\"  eval =\", evaluate_equation(_eq))\n",
    "print(\"  reward_exact =\", compute_reward(_txt, 13, [2, 3, 7], FORMAT_SCORE, True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9e1e9f72",
   "metadata": {},
   "source": [
    "## 4) Few-shot prompt + dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a57d3bb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T11:21:43.915338Z",
     "iopub.status.busy": "2026-01-27T11:21:43.915194Z",
     "iopub.status.idle": "2026-01-27T11:21:46.012868Z",
     "shell.execute_reply": "2026-01-27T11:21:46.012362Z",
     "shell.execute_reply.started": "2026-01-27T11:21:43.915326Z"
    },
    "tags": []
   },
   "source": [
    "COUNTDOWN_FEWSHOT = (\n",
    "    \"Q: Using the numbers 2, 3, 7, reach the target number 13. \"\n",
    "    \"You may use +, -, *, / and parentheses, and each number can only be used once. \"\n",
    "    \"Put ONLY the final expression inside <answer>...</answer>. \"\n",
    "    \"Example: <answer>(1+2)/3</answer>.\\n\"\n",
    "    \"A: <answer>(2*3)+7</answer>\\n\\n\"\n",
    ")\n",
    "\n",
    "# Stop when </answer> is generated (we still decode whole sequence returned by backend)\n",
    "STOP_SEQS = [\"</answer>\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Problem:\n",
    "    question: str\n",
    "    target: int\n",
    "    nums: list[int]\n",
    "\n",
    "\n",
    "class CountdownDatasetLoader:\n",
    "    \"\"\"Simple dataset wrapper with sequential batching for train/test.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_name: str, test_size: int, seed: int):\n",
    "        train_ds, test_ds = load_countdown_splits(\n",
    "            dataset_name=dataset_name,\n",
    "            split=\"train\",\n",
    "            test_size=test_size,\n",
    "            seed=seed,\n",
    "        )\n",
    "        self.train = train_ds\n",
    "        self.test = test_ds\n",
    "        self.train_idx = 0\n",
    "        self.test_idx = 0\n",
    "\n",
    "    def get_batch(self, batch_size: int, split: str = \"train\") -> list[Problem]:\n",
    "        ds = self.train if split == \"train\" else self.test\n",
    "        idx = self.train_idx if split == \"train\" else self.test_idx\n",
    "\n",
    "        problems: list[Problem] = []\n",
    "        for _ in range(batch_size):\n",
    "            if idx >= len(ds):\n",
    "                idx = 0\n",
    "            row = ds[idx]\n",
    "            idx += 1\n",
    "            problems.append(\n",
    "                Problem(\n",
    "                    question=f\"Q: {row['question']}\\nA:\",\n",
    "                    target=int(row[\"target\"]),\n",
    "                    nums=list(row[\"nums\"]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if split == \"train\":\n",
    "            self.train_idx = idx\n",
    "        else:\n",
    "            self.test_idx = idx\n",
    "        return problems\n",
    "\n",
    "\n",
    "dataset = CountdownDatasetLoader(DATASET_NAME, TEST_SIZE, SEED)\n",
    "sample_prob = dataset.get_batch(1, split=\"train\")[0]\n",
    "\n",
    "print(\"Few-shot chars:\", len(COUNTDOWN_FEWSHOT), \"| Stop:\", STOP_SEQS)\n",
    "print(\"Sample target:\", sample_prob.target, \"| nums:\", sample_prob.nums)\n",
    "print(\"Sample prompt snippet:\", (COUNTDOWN_FEWSHOT + sample_prob.question)[:140], \"...\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c4bb5bd7",
   "metadata": {},
   "source": [
    "## 5) Connect to TuFT server + create LoRA training client"
   ]
  },
  {
   "cell_type": "code",
   "id": "b6f9e22a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T11:21:46.014153Z",
     "iopub.status.busy": "2026-01-27T11:21:46.013967Z",
     "iopub.status.idle": "2026-01-27T11:21:49.909313Z",
     "shell.execute_reply": "2026-01-27T11:21:49.908767Z",
     "shell.execute_reply.started": "2026-01-27T11:21:46.014138Z"
    },
    "tags": []
   },
   "source": [
    "service_client = tinker.ServiceClient(base_url=TINKER_BASE_URL, api_key=TINKER_API_KEY)\n",
    "print(\"ServiceClient created:\", service_client)\n",
    "\n",
    "training_client = service_client.create_lora_training_client(\n",
    "    base_model=BASE_MODEL,\n",
    "    rank=LORA_RANK,\n",
    "    train_mlp=True,\n",
    "    train_attn=True,\n",
    "    train_unembed=True,\n",
    ")\n",
    "print(\"Training client created:\", training_client)\n",
    "print(\"LoRA config:\", {\"base_model\": BASE_MODEL, \"rank\": LORA_RANK})\n",
    "\n",
    "tokenizer = training_client.get_tokenizer()\n",
    "print(\"Tokenizer type:\", type(tokenizer).__name__)\n",
    "\n",
    "\n",
    "def make_prompt_model_input(text: str) -> types.ModelInput:\n",
    "    \"\"\"Build ModelInput from text.\n",
    "\n",
    "    Prefer ModelInput.from_text() if available, otherwise encode with tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    if hasattr(types.ModelInput, \"from_text\"):\n",
    "        return types.ModelInput.from_text(text)\n",
    "\n",
    "    toks = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return types.ModelInput(chunks=[types.EncodedTextChunk(tokens=toks)])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4e2d6fc3",
   "metadata": {},
   "source": [
    "## 6) Sampling params + optimizer params"
   ]
  },
  {
   "cell_type": "code",
   "id": "44cbbf1b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T11:21:49.910188Z",
     "iopub.status.busy": "2026-01-27T11:21:49.909830Z",
     "iopub.status.idle": "2026-01-27T11:21:49.915271Z",
     "shell.execute_reply": "2026-01-27T11:21:49.914866Z",
     "shell.execute_reply.started": "2026-01-27T11:21:49.910172Z"
    },
    "tags": []
   },
   "source": [
    "sampling_params_train = types.SamplingParams(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    temperature=TEMPERATURE,\n",
    "    stop=STOP_SEQS,\n",
    ")\n",
    "\n",
    "sampling_params_eval = types.SamplingParams(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    temperature=EVAL_TEMPERATURE,\n",
    "    stop=STOP_SEQS,\n",
    ")\n",
    "\n",
    "adam_params = types.AdamParams(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    beta1=0.9,\n",
    "    beta2=0.95,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "print(\"Params set:\")\n",
    "print(\"  train_temp =\", TEMPERATURE)\n",
    "print(\"  eval_temp  =\", EVAL_TEMPERATURE)\n",
    "print(\"  seed       =\", SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2eb3dfe9",
   "metadata": {},
   "source": [
    "## 7) Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "id": "71c77f7a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T11:21:49.915898Z",
     "iopub.status.busy": "2026-01-27T11:21:49.915754Z",
     "iopub.status.idle": "2026-01-27T11:21:49.920321Z",
     "shell.execute_reply": "2026-01-27T11:21:49.919865Z",
     "shell.execute_reply.started": "2026-01-27T11:21:49.915885Z"
    },
    "tags": []
   },
   "source": [
    "def do_eval(step: int) -> float:\n",
    "    \"\"\"Evaluate current LoRA checkpoint by sampling on test split and computing mean reward.\"\"\"\n",
    "\n",
    "    eval_path = training_client.save_weights_for_sampler(name=f\"eval_{step:06d}\").result().path\n",
    "    eval_client = service_client.create_sampling_client(model_path=eval_path)\n",
    "\n",
    "    probs = dataset.get_batch(EVAL_BATCH_SIZE, split=\"test\")\n",
    "    rewards = []\n",
    "\n",
    "    for prob in probs:\n",
    "        prompt_text = COUNTDOWN_FEWSHOT + prob.question\n",
    "        prompt = make_prompt_model_input(prompt_text)\n",
    "\n",
    "        res = eval_client.sample(\n",
    "            prompt=prompt,\n",
    "            num_samples=EVAL_GROUP_SIZE,\n",
    "            sampling_params=sampling_params_eval,\n",
    "        ).result()\n",
    "\n",
    "        for seq in res.sequences:\n",
    "            toks = list(seq.tokens)\n",
    "            resp_text = tokenizer.decode(toks, skip_special_tokens=True)\n",
    "            r = compute_reward(\n",
    "                response_text=resp_text,\n",
    "                target=prob.target,\n",
    "                nums=prob.nums,\n",
    "                format_score=FORMAT_SCORE,\n",
    "                use_continuous_shaping=USE_CONTINUOUS_SHAPING,\n",
    "            )\n",
    "            rewards.append(float(r))\n",
    "\n",
    "    return sum(rewards) / max(1, len(rewards))\n",
    "\n",
    "\n",
    "print(\"Eval function ready.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d466cd5c",
   "metadata": {},
   "source": [
    "## 8) Train loop (GRPO-like)\n",
    "\n",
    "Per training step:\n",
    "- Sample `GROUP_SIZE` rollouts per problem\n",
    "- Compute centered+normalized advantages within each group\n",
    "- Build importance-sampling datums and run a LoRA update\n",
    "- Periodically evaluate and track EMA reward\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5f68a5f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T11:21:49.921063Z",
     "iopub.status.busy": "2026-01-27T11:21:49.920904Z",
     "iopub.status.idle": "2026-01-27T12:03:21.711854Z",
     "shell.execute_reply": "2026-01-27T12:03:21.711263Z",
     "shell.execute_reply.started": "2026-01-27T11:21:49.921051Z"
    },
    "scrolled": true,
    "tags": []
   },
   "source": [
    "ema_eval_reward = None\n",
    "metrics_history = []\n",
    "\n",
    "print(f\"Starting RL training: {NUM_STEPS} steps\")\n",
    "print(f\"Batch: {BATCH_SIZE}, Group: {GROUP_SIZE}, LR: {LEARNING_RATE}\\n\")\n",
    "\n",
    "for step in range(NUM_STEPS):\n",
    "    problems_P = dataset.get_batch(BATCH_SIZE, split=\"train\")\n",
    "\n",
    "    # Sync weights -> sampling client\n",
    "    save_result = training_client.save_weights_for_sampler(name=f\"rl_step_{step:06d}\").result()\n",
    "    sampling_path = save_result.path\n",
    "    sampling_client = service_client.create_sampling_client(model_path=sampling_path)\n",
    "\n",
    "    datums_D: list[types.Datum] = []\n",
    "    mean_rewards_P: list[float] = []\n",
    "    kept_rollouts = 0\n",
    "    skipped_problems = 0\n",
    "\n",
    "    # Sample + build datums\n",
    "    for prob in problems_P:\n",
    "        prompt_text = COUNTDOWN_FEWSHOT + prob.question\n",
    "        prompt = make_prompt_model_input(prompt_text)\n",
    "\n",
    "        sample_res = sampling_client.sample(\n",
    "            prompt=prompt,\n",
    "            num_samples=GROUP_SIZE,\n",
    "            sampling_params=sampling_params_train,\n",
    "        ).result()\n",
    "\n",
    "        rewards_G: list[float] = []\n",
    "        tokens_G_T: list[list[int]] = []\n",
    "        logprobs_G_T: list[list[float]] = []\n",
    "\n",
    "        for seq in sample_res.sequences:\n",
    "            toks = list(seq.tokens)\n",
    "            lps = seq.logprobs\n",
    "            if lps is None:\n",
    "                raise RuntimeError(\"Sampling did not return logprobs.\")\n",
    "\n",
    "            resp_text = tokenizer.decode(toks, skip_special_tokens=True)\n",
    "            r = compute_reward(\n",
    "                response_text=resp_text,\n",
    "                target=prob.target,\n",
    "                nums=prob.nums,\n",
    "                format_score=FORMAT_SCORE,\n",
    "                use_continuous_shaping=USE_CONTINUOUS_SHAPING,\n",
    "            )\n",
    "\n",
    "            rewards_G.append(float(r))\n",
    "            tokens_G_T.append(toks)\n",
    "            logprobs_G_T.append(list(lps))\n",
    "\n",
    "        mean_r = sum(rewards_G) / len(rewards_G)\n",
    "        mean_rewards_P.append(mean_r)\n",
    "\n",
    "        var_r = sum((r - mean_r) ** 2 for r in rewards_G) / max(1, len(rewards_G))\n",
    "        std_r = var_r**0.5\n",
    "        if std_r < 1e-8:\n",
    "            skipped_problems += 1\n",
    "            continue\n",
    "\n",
    "        advantages_G = [(r - mean_r) / (std_r + 1e-6) for r in rewards_G]\n",
    "\n",
    "        ob_len = prompt.length - 1\n",
    "\n",
    "        for toks, lps, adv in zip(tokens_G_T, logprobs_G_T, advantages_G, strict=True):\n",
    "            # model_input excludes final token because training usually predicts next token\n",
    "            model_input = prompt.append(types.EncodedTextChunk(tokens=toks[:-1]))\n",
    "\n",
    "            # Align lengths with model_input.length\n",
    "            target_tokens = [0] * ob_len + toks\n",
    "            padded_sampling_logprobs = [0.0] * ob_len + lps\n",
    "            padded_advantages = [0.0] * ob_len + [adv] * (model_input.length - ob_len)\n",
    "\n",
    "            assert (\n",
    "                model_input.length\n",
    "                == len(target_tokens)\n",
    "                == len(padded_sampling_logprobs)\n",
    "                == len(padded_advantages)\n",
    "            ), (\n",
    "                model_input.length,\n",
    "                len(target_tokens),\n",
    "                len(padded_sampling_logprobs),\n",
    "                len(padded_advantages),\n",
    "            )\n",
    "\n",
    "            target_tokens_t = torch.tensor(target_tokens, dtype=torch.long)\n",
    "            logprobs_t = torch.tensor(padded_sampling_logprobs, dtype=torch.float32)\n",
    "            advantages_t = torch.tensor(padded_advantages, dtype=torch.float32)\n",
    "\n",
    "            loss_fn_inputs = {\n",
    "                \"target_tokens\": TensorData.from_torch(target_tokens_t),\n",
    "                \"logprobs\": TensorData.from_torch(logprobs_t),\n",
    "                \"advantages\": TensorData.from_torch(advantages_t),\n",
    "            }\n",
    "\n",
    "            datums_D.append(\n",
    "                types.Datum(\n",
    "                    model_input=model_input,\n",
    "                    loss_fn_inputs=loss_fn_inputs,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            kept_rollouts += 1\n",
    "\n",
    "    mean_reward_train = sum(mean_rewards_P) / max(1, len(mean_rewards_P))\n",
    "\n",
    "    # Optimization step\n",
    "    if datums_D:\n",
    "        _ = training_client.forward_backward(datums_D, loss_fn=\"importance_sampling\").result()\n",
    "        _ = training_client.optim_step(adam_params).result()\n",
    "\n",
    "    # Eval + EMA\n",
    "    eval_reward = None\n",
    "    ema_now = None\n",
    "    if EVAL_EVERY > 0 and (step % EVAL_EVERY == 0):\n",
    "        eval_reward = do_eval(step)\n",
    "        if ema_eval_reward is None:\n",
    "            ema_eval_reward = eval_reward\n",
    "        else:\n",
    "            a = REWARD_EMA_ALPHA\n",
    "            ema_eval_reward = (1 - a) * ema_eval_reward + a * eval_reward\n",
    "        ema_now = ema_eval_reward\n",
    "\n",
    "    metrics_history.append(\n",
    "        {\n",
    "            \"step\": int(step),\n",
    "            \"train_mean_reward\": float(mean_reward_train),\n",
    "            \"eval_mean_reward\": None if eval_reward is None else float(eval_reward),\n",
    "            \"ema_eval_reward\": None if ema_now is None else float(ema_now),\n",
    "            \"kept_rollouts\": int(kept_rollouts),\n",
    "            \"skipped_problems\": int(skipped_problems),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if step == 0:\n",
    "        delta_train = 0.0\n",
    "    else:\n",
    "        prev = metrics_history[-2][\"train_mean_reward\"]\n",
    "        delta_train = mean_reward_train - prev\n",
    "\n",
    "    if eval_reward is None:\n",
    "        print(\n",
    "            f\"Step {step:4d}: train_mean_reward={mean_reward_train:.4f} (Δ {delta_train:+.4f}) \"\n",
    "            f\"kept_rollouts={kept_rollouts} skipped_problems={skipped_problems}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Step {step:4d}: train_mean_reward={mean_reward_train:.4f} (Δ {delta_train:+.4f}) \"\n",
    "            f\"eval_mean_reward={eval_reward:.4f} ema_eval_reward={ema_now:.4f} \"\n",
    "            f\"kept_rollouts={kept_rollouts} skipped_problems={skipped_problems}\"\n",
    "        )\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "if metrics_history:\n",
    "    print(\"Initial train_mean_reward:\", f\"{metrics_history[0]['train_mean_reward']:.4f}\")\n",
    "    print(\"Final train_mean_reward:  \", f\"{metrics_history[-1]['train_mean_reward']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1cf8f67d",
   "metadata": {},
   "source": [
    "## 9) Final evaluation + plotting + checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "id": "6a6d5e58",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2026-01-27T12:11:48.409959Z",
     "iopub.status.busy": "2026-01-27T12:11:48.409730Z",
     "iopub.status.idle": "2026-01-27T12:11:51.756368Z",
     "shell.execute_reply": "2026-01-27T12:11:51.755834Z",
     "shell.execute_reply.started": "2026-01-27T12:11:48.409943Z"
    },
    "tags": []
   },
   "source": [
    "final_path = training_client.save_weights_for_sampler(name=\"COUNTDOWN-final\").result().path\n",
    "final_client = service_client.create_sampling_client(model_path=final_path)\n",
    "\n",
    "test_problems = dataset.get_batch(1, split=\"test\")\n",
    "\n",
    "print(\"Final Evaluation:\")\n",
    "print(\"=\" * 60)\n",
    "correct = 0\n",
    "\n",
    "# Some backends dislike temperature=0.0; keep a safe fallback.\n",
    "greedy_temp = 0.0\n",
    "\n",
    "for i, problem in enumerate(test_problems):\n",
    "    prompt_text = COUNTDOWN_FEWSHOT + problem.question\n",
    "    prompt_input = make_prompt_model_input(prompt_text)\n",
    "\n",
    "    try:\n",
    "        sampling_params_greedy = types.SamplingParams(\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            temperature=greedy_temp,\n",
    "            stop=STOP_SEQS,\n",
    "        )\n",
    "        result = final_client.sample(\n",
    "            prompt=prompt_input,\n",
    "            num_samples=1,\n",
    "            sampling_params=sampling_params_greedy,\n",
    "        ).result()\n",
    "    except Exception as e:\n",
    "        print(\"Greedy temp=0.0 failed, retrying with temp=0.1. Error:\", repr(e))\n",
    "        sampling_params_greedy = types.SamplingParams(\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            temperature=0.1,\n",
    "            stop=STOP_SEQS,\n",
    "        )\n",
    "        result = final_client.sample(\n",
    "            prompt=prompt_input,\n",
    "            num_samples=1,\n",
    "            sampling_params=sampling_params_greedy,\n",
    "        ).result()\n",
    "\n",
    "    response = tokenizer.decode(result.sequences[0].tokens, skip_special_tokens=True)\n",
    "    reward = compute_reward(\n",
    "        response_text=response,\n",
    "        target=problem.target,\n",
    "        nums=problem.nums,\n",
    "        format_score=FORMAT_SCORE,\n",
    "        use_continuous_shaping=USE_CONTINUOUS_SHAPING,\n",
    "    )\n",
    "\n",
    "    if reward >= 1.0:\n",
    "        correct += 1\n",
    "\n",
    "    status = \"PASS\" if reward >= 1.0 else \"FAIL\"\n",
    "    print(f\"[{i}] {problem.question[:100]}...\")\n",
    "    print(f\"A: {response.strip()[:140]}... [{status}] reward={reward:.4f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Plot eval mean reward (EMA optional)\n",
    "eval_ms = [m for m in metrics_history if m.get(\"eval_mean_reward\") is not None]\n",
    "eval_steps = [m[\"step\"] for m in eval_ms]\n",
    "eval_rewards = [m[\"eval_mean_reward\"] for m in eval_ms]\n",
    "\n",
    "ema_ms = [m for m in metrics_history if m.get(\"ema_eval_reward\") is not None]\n",
    "ema_steps = [m[\"step\"] for m in ema_ms]\n",
    "ema_rewards = [m[\"ema_eval_reward\"] for m in ema_ms]\n",
    "\n",
    "print(\"Plot points (eval):\", len(eval_steps))\n",
    "if len(eval_steps) > 0:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(eval_steps, eval_rewards, \"b-\", linewidth=2, label=\"eval_mean_reward\")\n",
    "    if len(ema_steps) > 0:\n",
    "        plt.plot(ema_steps, ema_rewards, \"r-\", linewidth=2, alpha=0.9, label=\"ema_eval_reward\")\n",
    "\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Eval mean reward\")\n",
    "    plt.title(\"COUNTDOWN RL Training\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_path = \"COUNTDOWN_training.png\"\n",
    "    plt.savefig(fig_path, dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No eval points available to plot. (Did you set EVAL_EVERY > 0?)\")\n",
    "\n",
    "\n",
    "checkpoint = training_client.save_state(name=\"COUNTDOWN-rl-final\").result()\n",
    "print(\"Checkpoint:\", checkpoint.path)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
